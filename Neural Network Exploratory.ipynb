{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import backend as K\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "#import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import hamming_loss, precision_score, recall_score, f1_score\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('id_genre_pairs.csv')\n",
    "movie_ids = list(data['movie_id'])\n",
    "data = np.array(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "files = [i for i in os.listdir('Movie Posters Resized 32x32') if i[-3:] == 'jpg']\n",
    "random.shuffle(files)\n",
    "\n",
    "training_indices = files[:int(len(files)*0.8)]\n",
    "test_indices = files[int(len(files)*0.8):]\n",
    "\n",
    "test = []\n",
    "test_labels = []\n",
    "for i in test_indices:\n",
    "    try:\n",
    "        img = load_img('Movie Posters Resized 32x32/'+i)\n",
    "        test.append(img_to_array(img))\n",
    "        idx = int(i[:i.index(\".\")])\n",
    "        for row in data:\n",
    "            if row[0] == idx:\n",
    "                test_labels.append(row[1:])\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "train_labels = []\n",
    "for i in training_indices:\n",
    "    #try:\n",
    "    img = load_img('Movie Posters Resized 32x32/'+i)\n",
    "    train.append(img_to_array(img))\n",
    "    idx = int(i[:i.index(\".\")])\n",
    "    for row in data:\n",
    "        if row[0] == idx:\n",
    "            train_labels.append(row[1:])\n",
    "            break\n",
    "    #except:\n",
    "        #pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 32, 32\n",
    "num_classes = 17\n",
    "\n",
    "x_train = np.array(train) \n",
    "x_test = np.array(test)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (11757, 32, 32, 3)\n",
      "x_test shape: (2940, 32, 32, 3)\n",
      "11757 train samples\n",
      "2940 test samples\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# interestingly the keras example code does not center the data\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Loss Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred, weights):\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = K.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = K.max(y_pred, axis=1)\n",
    "    y_pred_max = K.expand_dims(y_pred_max, 1)\n",
    "    y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "        final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\n",
    "    return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)* final_mask\n",
    "\n",
    "    \n",
    "w_array = np.ones((17,17))\n",
    "for i in range(17):\n",
    "    w_array[9, i] = 10\n",
    "\n",
    "ncce = partial(weighted_binary_crossentropy, weights=np.ones((17,17)))\n",
    "ncce.__name__ ='weighted_binary_crossentropy'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11757 samples, validate on 2940 samples\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0 is illegal; using conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0 is illegal; using conv2d_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "Epoch 1/20\n",
      "11757/11757 [==============================] - 12s - loss: 0.5623 - acc: 0.2152 - accuracy_with_threshold: 0.5285 - val_loss: 0.4785 - val_acc: 0.2939 - val_accuracy_with_threshold: 0.6806\n",
      "Epoch 2/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4733 - acc: 0.2829 - accuracy_with_threshold: 0.6508 - val_loss: 0.4647 - val_acc: 0.2935 - val_accuracy_with_threshold: 0.6873\n",
      "Epoch 3/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4602 - acc: 0.2833 - accuracy_with_threshold: 0.6739 - val_loss: 0.4561 - val_acc: 0.2874 - val_accuracy_with_threshold: 0.6727\n",
      "Epoch 4/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4523 - acc: 0.2836 - accuracy_with_threshold: 0.6832 - val_loss: 0.4494 - val_acc: 0.2908 - val_accuracy_with_threshold: 0.6625\n",
      "Epoch 5/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4451 - acc: 0.2837 - accuracy_with_threshold: 0.6887 - val_loss: 0.4442 - val_acc: 0.2901 - val_accuracy_with_threshold: 0.7012\n",
      "Epoch 6/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4385 - acc: 0.2890 - accuracy_with_threshold: 0.6981 - val_loss: 0.4386 - val_acc: 0.2942 - val_accuracy_with_threshold: 0.7122\n",
      "Epoch 7/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4349 - acc: 0.2905 - accuracy_with_threshold: 0.6857 - val_loss: 0.4355 - val_acc: 0.2929 - val_accuracy_with_threshold: 0.7276\n",
      "Epoch 8/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4296 - acc: 0.2933 - accuracy_with_threshold: 0.7020 - val_loss: 0.4320 - val_acc: 0.2946 - val_accuracy_with_threshold: 0.7145\n",
      "Epoch 9/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4248 - acc: 0.2938 - accuracy_with_threshold: 0.7000 - val_loss: 0.4293 - val_acc: 0.2935 - val_accuracy_with_threshold: 0.7064\n",
      "Epoch 10/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4214 - acc: 0.2944 - accuracy_with_threshold: 0.7003 - val_loss: 0.4279 - val_acc: 0.2959 - val_accuracy_with_threshold: 0.6773\n",
      "Epoch 11/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4184 - acc: 0.2949 - accuracy_with_threshold: 0.7037 - val_loss: 0.4270 - val_acc: 0.2935 - val_accuracy_with_threshold: 0.7328\n",
      "Epoch 12/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4158 - acc: 0.2971 - accuracy_with_threshold: 0.7037 - val_loss: 0.4235 - val_acc: 0.2939 - val_accuracy_with_threshold: 0.7274\n",
      "Epoch 13/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4122 - acc: 0.2940 - accuracy_with_threshold: 0.7111 - val_loss: 0.4212 - val_acc: 0.2915 - val_accuracy_with_threshold: 0.6924\n",
      "Epoch 14/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4106 - acc: 0.3018 - accuracy_with_threshold: 0.7078 - val_loss: 0.4225 - val_acc: 0.2918 - val_accuracy_with_threshold: 0.6796\n",
      "Epoch 15/20\n",
      "11757/11757 [==============================] - 13s - loss: 0.4076 - acc: 0.3002 - accuracy_with_threshold: 0.7114 - val_loss: 0.4205 - val_acc: 0.2939 - val_accuracy_with_threshold: 0.6741\n",
      "Epoch 16/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4066 - acc: 0.3025 - accuracy_with_threshold: 0.7093 - val_loss: 0.4198 - val_acc: 0.2908 - val_accuracy_with_threshold: 0.7411\n",
      "Epoch 17/20\n",
      "11757/11757 [==============================] - 10s - loss: 0.4043 - acc: 0.3015 - accuracy_with_threshold: 0.7139 - val_loss: 0.4189 - val_acc: 0.2942 - val_accuracy_with_threshold: 0.7239\n",
      "Epoch 18/20\n",
      "11757/11757 [==============================] - 10s - loss: 0.4025 - acc: 0.3071 - accuracy_with_threshold: 0.7152 - val_loss: 0.4169 - val_acc: 0.2959 - val_accuracy_with_threshold: 0.7117\n",
      "Epoch 19/20\n",
      "11757/11757 [==============================] - 11s - loss: 0.4010 - acc: 0.3061 - accuracy_with_threshold: 0.7177 - val_loss: 0.4159 - val_acc: 0.2915 - val_accuracy_with_threshold: 0.7115\n",
      "Epoch 20/20\n",
      "11757/11757 [==============================] - 10s - loss: 0.3993 - acc: 0.3094 - accuracy_with_threshold: 0.7193 - val_loss: 0.4170 - val_acc: 0.2922 - val_accuracy_with_threshold: 0.6718\n",
      "1 1 0.001\n",
      "Test loss: 0.416983310098\n",
      "Test accuracy: 0.292176870748\n",
      "Precision:  0.167308004986 micro,  0.103514901588  macro\n",
      "Recall:  0.748363636364 micro,  0.473896632376 macro \n",
      "F1-score: 0.273476257973 micro,  0.160763629524 macro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/courtneycochrane/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/courtneycochrane/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def accuracy_with_threshold(y_true, y_pred):\n",
    "    threshold = 0.1\n",
    "    y_pred = K.cast(K.greater(y_pred, threshold), K.floatx())\n",
    "    return K.mean(K.equal(y_true, y_pred))\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "#batch_size = 256\n",
    "batch_size=512 # Just for running locally - change this!\n",
    "# number of iterations over the complete training data\n",
    "epochs = 20\n",
    "#epochs=1 # Just for testing - change this!\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "parameters = []\n",
    "model_histories = []\n",
    "# CV for number of layers, number of dense layers, sgd vs. adam, regularization constant, and learning rate \n",
    "for n_conv_layers in [1]:\n",
    "    for n_dense_layers in [1]:\n",
    "        for lrate in [0.001]:\n",
    "            for adaptive_change in [0.01]:\n",
    "                log_dir_string = \"cv\"\n",
    "                log_dir_string += str(lrate)\n",
    "                log_dir_string += str(adaptive_change)\n",
    "                K.clear_session()\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(16, kernel_size=(5, 5), \n",
    "                                 activation='relu', \n",
    "                                 kernel_regularizer=regularizers.l2(0.01),\n",
    "                                 input_shape=input_shape))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                if n_conv_layers == 1:\n",
    "                    log_dir_string += \"_1conv\"\n",
    "                elif n_conv_layers==2:\n",
    "                    model.add(Conv2D(16, kernel_size=(5, 5), \n",
    "                                     activation='relu', \n",
    "                                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                                     input_shape=input_shape))\n",
    "                    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                    log_dir_string += \"_2conv\"\n",
    "                else:\n",
    "                    model.add(Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "                    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                    log_dir_string += \"_3conv\"\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(64, activation='relu'))\n",
    "                if n_dense_layers == 1:\n",
    "                    log_dir_string += \"_1fc\"\n",
    "                else:\n",
    "                    model.add(Dense(64, activation='relu'))\n",
    "                    log_dir_string += \"_2fc\"\n",
    "\n",
    "                model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "                adam = Adam(lr=lrate)\n",
    "                model.compile(loss=ncce,\n",
    "                      optimizer=adam,\n",
    "                      metrics=['accuracy',accuracy_with_threshold])\n",
    "\n",
    "                # we need a callback to save information for tensorboard visualizations\n",
    "                tensorboard = TensorBoard(log_dir='./logs/cv/'+log_dir_string, histogram_freq=1, write_graph=True, write_images=False)\n",
    "\n",
    "                reduce_lr = ReduceLROnPlateau(monitor='val_accuracy_with_threshold', factor = adaptive_change, patience = 5, min_lr = 0.00001)\n",
    "\n",
    "                history = model.fit(x_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            callbacks=[tensorboard, reduce_lr])\n",
    "\n",
    "\n",
    "\n",
    "                score = model.evaluate(x_test, y_test, verbose=0)\n",
    "                predictions = (model.predict(x_test)>0.1).astype(int)\n",
    "                print(n_conv_layers, n_dense_layers, lrate)\n",
    "                parameters.append([n_conv_layers, n_dense_layers, lrate])\n",
    "               \n",
    "                model_histories.append(history)\n",
    "                precision = precision_score(y_test, predictions, average=\"micro\")\n",
    "                recall = recall_score(y_test, predictions, average=\"micro\")\n",
    "                f1 = f1_score(y_test, predictions, average=\"micro\")\n",
    "                recalls.append(recall)\n",
    "                precisions.append(precision)\n",
    "                f1s.append(f1)\n",
    "                \n",
    "                print('Test loss:', score[0])\n",
    "                print('Test accuracy:', score[1])\n",
    "                print(\"Precision: \", precision, \"micro, \", precision_score(y_test, predictions, average=\"macro\"), \" macro\", )\n",
    "                print(\"Recall: \", recall, \"micro, \", recall_score(y_test, predictions, average=\"macro\"), \"macro \") \n",
    "                print(\"F1-score:\", f1_score(y_test, predictions, average=\"micro\"), \"micro, \", f1_score(y_test, predictions, average=\"macro\"), \"macro\")\n",
    "                \n",
    "            \n",
    "\n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
