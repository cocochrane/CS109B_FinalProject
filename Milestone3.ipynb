{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline: Logistic Regression\n",
    "\n",
    "#### SVM\n",
    "\n",
    "We selected an SVM as these models generally produce very good classifications and they are robust to noise and less prone to overfitting. The major disadvantage to the SVM, which we ran into, is that it's computationally expensive. We tried to speed up tuning time by running PCA and retaining the principle components that accounted for 90% of the variance. We also tried randomly sampling a proportion of the training data with which to tune on. We ran the SVM using five-fold cross validdation and performed grid search with the parameters kernel (rbf, linear), gamma, and cost. \n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "We selected to use a Random Forest as an alternative to the SVM, both for its general classification accuracy and relative training efficiency. Random forests are generally good candidates for classification tasks due to their ability to combine multiple predictions into an ensemble in order to reduce model variance. However, random forests are also susceptible to overfitting, particularly in instances where the data are sparse. In this dataset, we retained several numerical features from the original TMDB/IMDB datasets, and constructed bag-of-words representations of the titles and overviews, retaining the top 100 most frequent words for each feature. While still sparse, we expected that retaining only the most frequent words would reduce the overall sparsity and allow the RF to perform relatively well. \n",
    "\n",
    "In order to reduce overfitting, we tuned the Random Forest using five-fold cross validation and performed grid search using three values each for max_features, max_depth, and num_estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "F1 measures the balance between precision (exactness) and recall (sensitivity) scores and is calculated as:\n",
    "\n",
    "$$\\frac{2*(precision*recall)}{precision + recall}$$\n",
    "\n",
    "For this task, we are concerned by the imbalance among the genres in the dataset. We determined that one way to account for this problem during was to use the F-score during model tuning; by balancing precision and recall, the model with the \"best performance\" will not be one that selectively performs well only on the dominant class.\n",
    "\n",
    "#### Hamming Loss\n",
    "\n",
    "This metric measures model accuracy for multi-label classification problems. Although we implemented multi-class models above, we constructed \"multi-label\" final outputs by combining each genre classifier's predicted output into a multi-label Y. We then computed the hamming loss on this final output in order to compare the three strategies (Logistic Regression, RF, and SVM).\n",
    "\n",
    "Hamming loss is given by the following formula:\n",
    "\n",
    "$$\\frac{1}{|D|}\\sum_{i=1}^{|D|} \\frac{xor(x_i,y_i)}{|L|}$$\n",
    "\n",
    "where $|D|$ is the number of observations, $|L|$ is the number of labels, $y_i$ are the actual labels, and $x_i$ are the predicted labels.\n",
    "\n",
    "Source: (https://www.kaggle.com/wiki/HammingLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following scores are reported for the test set:\n",
    "\n",
    "|        Model        |  Hamming Loss  |\n",
    "|---------------------|----------------|\n",
    "| Logistic Regression |      0.21      |\n",
    "|    Random Forest    |      0.07      |\n",
    "|         SVM         |      0.23      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The SVM model performed the worst in terms of Hamming loss of the three models. WE attribute this to the fact that the SVM was too computationally expensive to tune with a finer grid-search. Further, while sampling did speed up the computation time, it decreased the f1 score of particularly the more obscure genres that are not common in our dataset. In terms of model improvement there are two clear paths forward. Firstly, the results would almost certainly improve if we used a wider array of values for the hyperparameters in tuning. Secondly, sampling could be preformed in a stratified manner, thus decreasing the computation time while maintaining enough of each genre to make an accurate classifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
